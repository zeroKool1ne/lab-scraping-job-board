{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Board Scraping Lab\n",
    "\n",
    "In this lab you will first see a minimal but fully functional code snippet to scrape the LinkedIn Job Search webpage. You will then work on top of the example code and complete several chanllenges.\n",
    "\n",
    "### Some Resources \n",
    "\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the required libraries\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport requests\nimport time\n\n# Set a User-Agent header so LinkedIn doesn't block us\nHEADERS = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n}\n\n\"\"\"\nThis function searches job posts from LinkedIn and converts the results into a dataframe.\nLinkedIn's public search page now uses updated CSS classes:\n  - Card container: div.base-search-card\n  - Title: h3.base-search-card__title\n  - Company: h4.base-search-card__subtitle\n  - Location: span.job-search-card__location\n\"\"\"\ndef scrape_linkedin_job_search(keywords):\n    \n    # Define the base url to be scraped\n    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n    \n    # Assemble the full url with parameters\n    scrape_url = ''.join([BASE_URL, 'keywords=', keywords])\n\n    # Create a request to get the data from the server \n    page = requests.get(scrape_url, headers=HEADERS)\n    soup = BeautifulSoup(page.text, 'html.parser')\n\n    # Retrieve HTML code from the webpage. Parse the HTML into a list of \"cards\".\n    # Then in each job card, extract the job title, company, and location data.\n    titles = []\n    companies = []\n    locations = []\n    for card in soup.select(\"div.base-search-card__info\"):\n        title = card.select_one(\"h3.base-search-card__title\")\n        company = card.select_one(\"h4.base-search-card__subtitle\")\n        location = card.select_one(\"span.job-search-card__location\")\n        titles.append(title.get_text(strip=True) if title else None)\n        companies.append(company.get_text(strip=True) if company else None)\n        locations.append(location.get_text(strip=True) if location else None)\n    \n    # Build the dataframe from the collected data\n    data = pd.DataFrame({'Title': titles, 'Company': companies, 'Location': locations})\n    \n    # Return dataframe\n    return data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example to call the function\n\nresults = scrape_linkedin_job_search('data%20analyst')\nprint(f\"Total jobs found: {len(results)}\")\nresults.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "The first challenge for you is to update the `scrape_linkedin_job_search` function by adding a new parameter called `num_pages`. This will allow you to search more than 25 jobs with this function. Suggested steps:\n",
    "\n",
    "1. Go to https://www.linkedin.com/jobs/search/?keywords=data%20analysis in your browser.\n",
    "1. Scroll down the left panel and click the page 2 link. Look at how the URL changes and identify the page offset parameter.\n",
    "1. Add `num_pages` as a new param to the `scrape_linkedin_job_search` function. Update the function code so that it uses a \"for\" loop to retrieve several pages of search results.\n",
    "1. Test your new function by scraping 5 pages of the search results.\n",
    "\n",
    "Hint: Prepare for the case where there are less than 5 pages of search results. Your function should be robust enough to **not** trigger errors. Simply skip making additional searches and return all results if the search already reaches the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Challenge 1: Add num_pages parameter for pagination\n# LinkedIn loads ~25 results per \"page\". The first page is the main search URL.\n# Additional pages use the guest API endpoint with a 'start' offset.\n\ndef scrape_linkedin_job_search(keywords, num_pages=1):\n    \n    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n    PAGINATION_URL = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?'\n    \n    titles = []\n    companies = []\n    locations = []\n    \n    for page in range(num_pages):\n        start = page * 25\n        \n        if page == 0:\n            scrape_url = f\"{BASE_URL}keywords={keywords}\"\n        else:\n            scrape_url = f\"{PAGINATION_URL}keywords={keywords}&start={start}\"\n        \n        page_response = requests.get(scrape_url, headers=HEADERS)\n        soup = BeautifulSoup(page_response.text, 'html.parser')\n        \n        cards = soup.select(\"div.base-search-card__info\")\n        \n        # If no cards found, we've run out of results — stop early\n        if not cards:\n            break\n        \n        for card in cards:\n            title = card.select_one(\"h3.base-search-card__title\")\n            company = card.select_one(\"h4.base-search-card__subtitle\")\n            location = card.select_one(\"span.job-search-card__location\")\n            titles.append(title.get_text(strip=True) if title else None)\n            companies.append(company.get_text(strip=True) if company else None)\n            locations.append(location.get_text(strip=True) if location else None)\n        \n        time.sleep(0.5)  # Be polite — small delay between requests\n    \n    data = pd.DataFrame({'Title': titles, 'Company': companies, 'Location': locations})\n    return data\n\n# Test with 5 pages\nresults_c1 = scrape_linkedin_job_search('data%20analyst', num_pages=5)\nprint(f\"Total jobs found: {len(results_c1)}\")\nresults_c1.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Further improve your function so that it can search jobs in a specific country. Add the 3rd param to your function called `country`. The steps are identical to those in Challange 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Challenge 2: Add country parameter\n# LinkedIn accepts a 'location' query param to filter by country\n\ndef scrape_linkedin_job_search(keywords, num_pages=1, country=None):\n    \n    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n    PAGINATION_URL = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?'\n    \n    titles = []\n    companies = []\n    locations = []\n    \n    for page in range(num_pages):\n        start = page * 25\n        \n        if page == 0:\n            scrape_url = f\"{BASE_URL}keywords={keywords}\"\n        else:\n            scrape_url = f\"{PAGINATION_URL}keywords={keywords}&start={start}\"\n        \n        if country:\n            scrape_url += f\"&location={country}\"\n        \n        page_response = requests.get(scrape_url, headers=HEADERS)\n        soup = BeautifulSoup(page_response.text, 'html.parser')\n        \n        cards = soup.select(\"div.base-search-card__info\")\n        \n        if not cards:\n            break\n        \n        for card in cards:\n            title = card.select_one(\"h3.base-search-card__title\")\n            company = card.select_one(\"h4.base-search-card__subtitle\")\n            location = card.select_one(\"span.job-search-card__location\")\n            titles.append(title.get_text(strip=True) if title else None)\n            companies.append(company.get_text(strip=True) if company else None)\n            locations.append(location.get_text(strip=True) if location else None)\n        \n        time.sleep(0.5)\n    \n    data = pd.DataFrame({'Title': titles, 'Company': companies, 'Location': locations})\n    return data\n\n# Test: search for data analyst jobs in Germany, 3 pages\nresults_c2 = scrape_linkedin_job_search('data%20analyst', num_pages=3, country='Germany')\nprint(f\"Total jobs found: {len(results_c2)}\")\nresults_c2.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 3\n",
    "\n",
    "Add the 4th param called `num_days` to your function to allow it to search jobs posted in the past X days. Note that in the LinkedIn job search the searched timespan is specified with the following param:\n",
    "\n",
    "```\n",
    "f_TPR=r259200\n",
    "```\n",
    "\n",
    "The number part in the param value is the number of seconds. 259,200 seconds equal to 3 days. You need to convert `num_days` to number of seconds and supply that info to LinkedIn job search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Challenge 3: Add num_days parameter for time filtering\n# LinkedIn uses f_TPR=r{seconds} to filter by recency\n\ndef scrape_linkedin_job_search(keywords, num_pages=1, country=None, num_days=None):\n    \n    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n    PAGINATION_URL = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?'\n    \n    titles = []\n    companies = []\n    locations = []\n    \n    for page in range(num_pages):\n        start = page * 25\n        \n        if page == 0:\n            scrape_url = f\"{BASE_URL}keywords={keywords}\"\n        else:\n            scrape_url = f\"{PAGINATION_URL}keywords={keywords}&start={start}\"\n        \n        if country:\n            scrape_url += f\"&location={country}\"\n        \n        if num_days:\n            num_seconds = num_days * 24 * 60 * 60  # convert days to seconds\n            scrape_url += f\"&f_TPR=r{num_seconds}\"\n        \n        page_response = requests.get(scrape_url, headers=HEADERS)\n        soup = BeautifulSoup(page_response.text, 'html.parser')\n        \n        cards = soup.select(\"div.base-search-card__info\")\n        \n        if not cards:\n            break\n        \n        for card in cards:\n            title = card.select_one(\"h3.base-search-card__title\")\n            company = card.select_one(\"h4.base-search-card__subtitle\")\n            location = card.select_one(\"span.job-search-card__location\")\n            titles.append(title.get_text(strip=True) if title else None)\n            companies.append(company.get_text(strip=True) if company else None)\n            locations.append(location.get_text(strip=True) if location else None)\n        \n        time.sleep(0.5)\n    \n    data = pd.DataFrame({'Title': titles, 'Company': companies, 'Location': locations})\n    return data\n\n# Test: search for data analyst jobs in Germany, 3 pages, posted in the last 7 days\nresults_c3 = scrape_linkedin_job_search('data%20analyst', num_pages=3, country='Germany', num_days=7)\nprint(f\"Total jobs found: {len(results_c3)}\")\nresults_c3.head(10)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge\n",
    "\n",
    "Allow your function to also retrieve the \"Seniority Level\" of each job searched. Note that the Seniority Level info is not in the initial search results. You need to make a separate search request for each job card based on the `currentJobId` value which you can extract from the job card HTML.\n",
    "\n",
    "After you obtain the Seniority Level info, update the function and add it to a new column of the returned dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bonus Challenge: Add Seniority Level column\n# Each job card has a data-entity-urn with the job ID.\n# We fetch each job's detail page and extract \"Seniority level\" from the criteria list.\n\ndef get_seniority_level(job_id):\n    \"\"\"Fetch an individual job page and extract the Seniority Level.\"\"\"\n    url = f\"https://www.linkedin.com/jobs/view/{job_id}\"\n    try:\n        response = requests.get(url, headers=HEADERS, timeout=10)\n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Find all criteria items and look for \"Seniority level\"\n        criteria_items = soup.select(\"li.description__job-criteria-item\")\n        for item in criteria_items:\n            header = item.select_one(\"h3.description__job-criteria-subheader\")\n            if header and \"seniority\" in header.get_text(strip=True).lower():\n                value = item.select_one(\"span.description__job-criteria-text--criteria\")\n                return value.get_text(strip=True) if value else None\n    except Exception:\n        pass\n    return None\n\n\ndef scrape_linkedin_job_search(keywords, num_pages=1, country=None, num_days=None, include_seniority=False):\n    \n    BASE_URL = 'https://www.linkedin.com/jobs/search/?'\n    PAGINATION_URL = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?'\n    \n    titles = []\n    companies = []\n    locations = []\n    job_ids = []\n    \n    for page in range(num_pages):\n        start = page * 25\n        \n        if page == 0:\n            scrape_url = f\"{BASE_URL}keywords={keywords}\"\n        else:\n            scrape_url = f\"{PAGINATION_URL}keywords={keywords}&start={start}\"\n        \n        if country:\n            scrape_url += f\"&location={country}\"\n        \n        if num_days:\n            num_seconds = num_days * 24 * 60 * 60\n            scrape_url += f\"&f_TPR=r{num_seconds}\"\n        \n        page_response = requests.get(scrape_url, headers=HEADERS)\n        soup = BeautifulSoup(page_response.text, 'html.parser')\n        \n        # Select the parent card (not just __info) so we can read data-entity-urn\n        cards = soup.select(\"div.base-search-card.job-search-card\")\n        \n        if not cards:\n            break\n        \n        for card in cards:\n            info = card.select_one(\"div.base-search-card__info\")\n            if not info:\n                continue\n            \n            title = info.select_one(\"h3.base-search-card__title\")\n            company = info.select_one(\"h4.base-search-card__subtitle\")\n            location = info.select_one(\"span.job-search-card__location\")\n            titles.append(title.get_text(strip=True) if title else None)\n            companies.append(company.get_text(strip=True) if company else None)\n            locations.append(location.get_text(strip=True) if location else None)\n            \n            # Extract job ID from data-entity-urn=\"urn:li:jobPosting:1234567\"\n            urn = card.get(\"data-entity-urn\", \"\")\n            job_id = urn.split(\":\")[-1] if urn else None\n            job_ids.append(job_id)\n        \n        time.sleep(0.5)\n    \n    data = pd.DataFrame({'Title': titles, 'Company': companies, 'Location': locations})\n    \n    # Optionally fetch seniority level for each job\n    if include_seniority:\n        print(f\"Fetching seniority level for {len(job_ids)} jobs (this may take a while)...\")\n        seniority_levels = []\n        for i, jid in enumerate(job_ids):\n            if jid:\n                seniority_levels.append(get_seniority_level(jid))\n                time.sleep(0.3)  # Be polite with rate limiting\n            else:\n                seniority_levels.append(None)\n            if (i + 1) % 10 == 0:\n                print(f\"  ...processed {i + 1}/{len(job_ids)} jobs\")\n        data['Seniority Level'] = seniority_levels\n    \n    return data\n\n# Test: search for data analyst jobs, 1 page, with seniority level\n# Using only 1 page to keep runtime reasonable since each job needs a separate request\nresults_bonus = scrape_linkedin_job_search('data%20analyst', num_pages=1, country='Germany', num_days=7, include_seniority=True)\nprint(f\"\\nTotal jobs found: {len(results_bonus)}\")\nresults_bonus.head(10)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}